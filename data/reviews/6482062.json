{
	"description": "math/big: faster (add|sub)V(V|W) routines\n\nBenchmarks run on 3.06GHz Intel Core 2 Duo,\n4GB 800MHz DDR2 SDRAM (\"iMac\").\n\nbenchmark             old ns/op    new ns/op    delta\nBenchmarkAddVV_1              6            6   +2.75%\nBenchmarkAddVV_2              9            7  -19.71%\nBenchmarkAddVV_3              9            9   +2.25%\nBenchmarkAddVV_4             10            8  -20.46%\nBenchmarkAddVV_5             12           10  -19.53%\nBenchmarkAddVV_1e1           23           15  -32.48%\nBenchmarkAddVV_1e2          213          107  -49.77%\nBenchmarkAddVV_1e3         2088          993  -52.44%\nBenchmarkAddVV_1e4        20874        12027  -42.38%\nBenchmarkAddVV_1e5       209858       121480  -42.11%\nBenchmarkAddVW_1              5            5   +0.90%\nBenchmarkAddVW_2             11           11   -3.51%\nBenchmarkAddVW_3              7            7   -0.27%\nBenchmarkAddVW_4              8            7   -6.32%\nBenchmarkAddVW_5              9            8  -10.89%\nBenchmarkAddVW_1e1           17           12  -26.01%\nBenchmarkAddVW_1e2          155           89  -42.32%\nBenchmarkAddVW_1e3         1479          873  -40.97%\nBenchmarkAddVW_1e4        13838         8764  -36.67%\nBenchmarkAddVW_1e5       147353        89560  -39.22%\n\nbenchmark              old MB/s     new MB/s  speedup\nBenchmarkAddVV_1        9765.57      9508.55    0.97x\nBenchmarkAddVV_2       13077.63     16284.97    1.25x\nBenchmarkAddVV_3       20599.58     20156.67    0.98x\nBenchmarkAddVV_4       23591.58     29516.02    1.25x\nBenchmarkAddVV_5       24920.95     31194.10    1.25x\nBenchmarkAddVV_1e1     27393.76     40621.71    1.48x\nBenchmarkAddVV_1e2     29911.96     59592.99    1.99x\nBenchmarkAddVV_1e3     30650.73     64429.84    2.10x\nBenchmarkAddVV_1e4     30660.09     53213.08    1.74x\nBenchmarkAddVV_1e5     30496.74     52683.46    1.73x\nBenchmarkAddVW_1       11503.39     11405.98    0.99x\nBenchmarkAddVW_2       11203.56     11586.92    1.03x\nBenchmarkAddVW_3       26173.45     26224.75    1.00x\nBenchmarkAddVW_4       30560.30     32621.94    1.07x\nBenchmarkAddVW_5       33183.81     37269.94    1.12x\nBenchmarkAddVW_1e1     36991.75     50098.53    1.35x\nBenchmarkAddVW_1e2     41087.14     71549.93    1.74x\nBenchmarkAddVW_1e3     43266.42     73279.83    1.69x\nBenchmarkAddVW_1e4     46246.74     73021.97    1.58x\nBenchmarkAddVW_1e5     43433.00     71459.96    1.65x\n\nBenchmarks run on 2.8GHz Quad-Code Intel Xeon,\n4GB 800MHz DDR2 FB-DIMM (\"PowerMac\").\n\nbenchmark             old ns/op    new ns/op    delta\nBenchmarkAddVV_1              7            7   +2.51%\nBenchmarkAddVV_2              8            8   +3.70%\nBenchmarkAddVV_3             10           10   +4.00%\nBenchmarkAddVV_4             11            9  -19.49%\nBenchmarkAddVV_5             14           11  -18.44%\nBenchmarkAddVV_1e1           23           17  -27.00%\nBenchmarkAddVV_1e2          234          117  -50.00%\nBenchmarkAddVV_1e3         2284         1095  -52.06%\nBenchmarkAddVV_1e4        22906        13149  -42.60%\nBenchmarkAddVV_1e5       229860       135133  -41.21%\nBenchmarkAddVW_1              6            6   +1.15%\nBenchmarkAddVW_2              7            7   +1.37%\nBenchmarkAddVW_3              7            8   +1.00%\nBenchmarkAddVW_4              9            8   -6.93%\nBenchmarkAddVW_5             10            9  -13.21%\nBenchmarkAddVW_1e1           18           14  -24.32%\nBenchmarkAddVW_1e2          170           97  -42.41%\nBenchmarkAddVW_1e3         1619          953  -41.14%\nBenchmarkAddVW_1e4        15142         9776  -35.44%\nBenchmarkAddVW_1e5       160835       102396  -36.33%\n\nbenchmark              old MB/s     new MB/s  speedup\nBenchmarkAddVV_1        8928.95      8702.84    0.97x\nBenchmarkAddVV_2       15298.84     14739.60    0.96x\nBenchmarkAddVV_3       19116.52     18375.37    0.96x\nBenchmarkAddVV_4       21644.30     26935.44    1.24x\nBenchmarkAddVV_5       22771.64     27754.04    1.22x\nBenchmarkAddVV_1e1     27017.62     37050.89    1.37x\nBenchmarkAddVV_1e2     27326.09     54289.15    1.99x\nBenchmarkAddVV_1e3     28016.84     58428.83    2.09x\nBenchmarkAddVV_1e4     27939.38     48670.55    1.74x\nBenchmarkAddVV_1e5     27843.00     47360.54    1.70x\nBenchmarkAddVW_1       10510.97     10397.27    0.99x\nBenchmarkAddVW_2       17499.71     17279.03    0.99x\nBenchmarkAddVW_3       24093.93     23858.39    0.99x\nBenchmarkAddVW_4       27733.08     29799.42    1.07x\nBenchmarkAddVW_5       30267.17     34781.83    1.15x\nBenchmarkAddVW_1e1     34566.78     45629.88    1.32x\nBenchmarkAddVW_1e2     37521.89     65341.93    1.74x\nBenchmarkAddVW_1e3     39513.18     67153.67    1.70x\nBenchmarkAddVW_1e4     42263.80     65464.60    1.55x\nBenchmarkAddVW_1e5     39792.21     62501.88    1.57x",
	"cc": [
		"iant@golang.org",
		"remyoudompheng@gmail.com",
		"nightlyone@googlemail.com",
		"minux.ma@gmail.com",
		"golang-dev@googlegroups.com"
	],
	"reviewers": [],
	"messages": [
		{
			"sender": "gri@golang.org",
			"recipients": [
				"gri@golang.org",
				"iant@golang.org",
				"remyoudompheng@gmail.com",
				"nightlyone@googlemail.com",
				"minux.ma@gmail.com",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "*** Submitted as http://code.google.com/p/go/source/detail?r=2507138fe625 ***\n\nmath/big: faster (add|sub)V(V|W) routines\n\nBenchmarks run on 3.06GHz Intel Core 2 Duo,\n4GB 800MHz DDR2 SDRAM (\"iMac\").\n\nbenchmark             old ns/op    new ns/op    delta\nBenchmarkAddVV_1              6            6   +2.75%\nBenchmarkAddVV_2              9            7  -19.71%\nBenchmarkAddVV_3              9            9   +2.25%\nBenchmarkAddVV_4             10            8  -20.46%\nBenchmarkAddVV_5             12           10  -19.53%\nBenchmarkAddVV_1e1           23           15  -32.48%\nBenchmarkAddVV_1e2          213          107  -49.77%\nBenchmarkAddVV_1e3         2088          993  -52.44%\nBenchmarkAddVV_1e4        20874        12027  -42.38%\nBenchmarkAddVV_1e5       209858       121480  -42.11%\nBenchmarkAddVW_1              5            5   +0.90%\nBenchmarkAddVW_2             11           11   -3.51%\nBenchmarkAddVW_3              7            7   -0.27%\nBenchmarkAddVW_4              8            7   -6.32%\nBenchmarkAddVW_5              9            8  -10.89%\nBenchmarkAddVW_1e1           17           12  -26.01%\nBenchmarkAddVW_1e2          155           89  -42.32%\nBenchmarkAddVW_1e3         1479          873  -40.97%\nBenchmarkAddVW_1e4        13838         8764  -36.67%\nBenchmarkAddVW_1e5       147353        89560  -39.22%\n\nbenchmark              old MB/s     new MB/s  speedup\nBenchmarkAddVV_1        9765.57      9508.55    0.97x\nBenchmarkAddVV_2       13077.63     16284.97    1.25x\nBenchmarkAddVV_3       20599.58     20156.67    0.98x\nBenchmarkAddVV_4       23591.58     29516.02    1.25x\nBenchmarkAddVV_5       24920.95     31194.10    1.25x\nBenchmarkAddVV_1e1     27393.76     40621.71    1.48x\nBenchmarkAddVV_1e2     29911.96     59592.99    1.99x\nBenchmarkAddVV_1e3     30650.73     64429.84    2.10x\nBenchmarkAddVV_1e4     30660.09     53213.08    1.74x\nBenchmarkAddVV_1e5     30496.74     52683.46    1.73x\nBenchmarkAddVW_1       11503.39     11405.98    0.99x\nBenchmarkAddVW_2       11203.56     11586.92    1.03x\nBenchmarkAddVW_3       26173.45     26224.75    1.00x\nBenchmarkAddVW_4       30560.30     32621.94    1.07x\nBenchmarkAddVW_5       33183.81     37269.94    1.12x\nBenchmarkAddVW_1e1     36991.75     50098.53    1.35x\nBenchmarkAddVW_1e2     41087.14     71549.93    1.74x\nBenchmarkAddVW_1e3     43266.42     73279.83    1.69x\nBenchmarkAddVW_1e4     46246.74     73021.97    1.58x\nBenchmarkAddVW_1e5     43433.00     71459.96    1.65x\n\nBenchmarks run on 2.8GHz Quad-Code Intel Xeon,\n4GB 800MHz DDR2 FB-DIMM (\"PowerMac\").\n\nbenchmark             old ns/op    new ns/op    delta\nBenchmarkAddVV_1              7            7   +2.51%\nBenchmarkAddVV_2              8            8   +3.70%\nBenchmarkAddVV_3             10           10   +4.00%\nBenchmarkAddVV_4             11            9  -19.49%\nBenchmarkAddVV_5             14           11  -18.44%\nBenchmarkAddVV_1e1           23           17  -27.00%\nBenchmarkAddVV_1e2          234          117  -50.00%\nBenchmarkAddVV_1e3         2284         1095  -52.06%\nBenchmarkAddVV_1e4        22906        13149  -42.60%\nBenchmarkAddVV_1e5       229860       135133  -41.21%\nBenchmarkAddVW_1              6            6   +1.15%\nBenchmarkAddVW_2              7            7   +1.37%\nBenchmarkAddVW_3              7            8   +1.00%\nBenchmarkAddVW_4              9            8   -6.93%\nBenchmarkAddVW_5             10            9  -13.21%\nBenchmarkAddVW_1e1           18           14  -24.32%\nBenchmarkAddVW_1e2          170           97  -42.41%\nBenchmarkAddVW_1e3         1619          953  -41.14%\nBenchmarkAddVW_1e4        15142         9776  -35.44%\nBenchmarkAddVW_1e5       160835       102396  -36.33%\n\nbenchmark              old MB/s     new MB/s  speedup\nBenchmarkAddVV_1        8928.95      8702.84    0.97x\nBenchmarkAddVV_2       15298.84     14739.60    0.96x\nBenchmarkAddVV_3       19116.52     18375.37    0.96x\nBenchmarkAddVV_4       21644.30     26935.44    1.24x\nBenchmarkAddVV_5       22771.64     27754.04    1.22x\nBenchmarkAddVV_1e1     27017.62     37050.89    1.37x\nBenchmarkAddVV_1e2     27326.09     54289.15    1.99x\nBenchmarkAddVV_1e3     28016.84     58428.83    2.09x\nBenchmarkAddVV_1e4     27939.38     48670.55    1.74x\nBenchmarkAddVV_1e5     27843.00     47360.54    1.70x\nBenchmarkAddVW_1       10510.97     10397.27    0.99x\nBenchmarkAddVW_2       17499.71     17279.03    0.99x\nBenchmarkAddVW_3       24093.93     23858.39    0.99x\nBenchmarkAddVW_4       27733.08     29799.42    1.07x\nBenchmarkAddVW_5       30267.17     34781.83    1.15x\nBenchmarkAddVW_1e1     34566.78     45629.88    1.32x\nBenchmarkAddVW_1e2     37521.89     65341.93    1.74x\nBenchmarkAddVW_1e3     39513.18     67153.67    1.70x\nBenchmarkAddVW_1e4     42263.80     65464.60    1.55x\nBenchmarkAddVW_1e5     39792.21     62501.88    1.57x\n\nR=iant, remyoudompheng, nightlyone, minux.ma\nCC=golang-dev\nhttp://codereview.appspot.com/6482062",
			"disapproval": false,
			"date": "2012-08-24 16:21:07.890830",
			"approval": false
		},
		{
			"sender": "minux.ma@gmail.com",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On Fri, Aug 24, 2012 at 8:32 PM, Ingo Oeser <nightlyone@googlemail.com>wrote:\r\n\r\n> Stupid question: Any reason this kind of low level optimization is not\r\n> done at some peep-hole optimizer stage in the compiler via instruction\r\n> pattern matching?\r\n>\r\nyou mean unrolling loops written in assembly programs? i think it would be\r\nmuch more difficult than\r\nunrolling loops in high level languages.\r\n\r\nas we don't have much assembly routines, adding such a optimizing pass\r\ndoesn't\r\nseem worthy (beside this pass will only have very limited applicability).\r\n\r\nif we add more optimizing pass, we'd better add it to gc.\r\n",
			"disapproval": false,
			"date": "2012-08-24 13:07:18.494600",
			"approval": false
		},
		{
			"sender": "gri@golang.org",
			"recipients": [
				"gri@golang.org",
				"iant@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Hello iant@golang.org (cc: golang-dev@googlegroups.com),\n\nI'd like you to review this change to\nhttps://go.googlecode.com/hg/",
			"disapproval": false,
			"date": "2012-08-24 06:17:42.413470",
			"approval": false
		},
		{
			"sender": "nightlyone@googlemail.com",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On Friday, August 24, 2012 3:06:57 PM UTC+2, minux wrote:\r\n>\r\n>\r\n> On Fri, Aug 24, 2012 at 8:32 PM, Ingo Oeser <night...@googlemail.com<javascript:>\r\n> > wrote:\r\n>\r\n>> Stupid question: Any reason this kind of low level optimization is not \r\n>> done at some peep-hole optimizer stage in the compiler via instruction \r\n>> pattern matching?\r\n>>\r\n> you mean unrolling loops written in assembly programs? \r\n\r\n\r\nThat would be a linker stage but would require code size and code speed \r\nmetrics to find a tradeoff.\r\n\r\n\r\n> as we don't have much assembly routines, adding such a optimizing pass \r\n> doesn't\r\n> seem worthy (beside this pass will only have very limited applicability).\r\n>\r\n>  \r\nThe idea was: We add highly optimized assembler routines for things we \r\ncould detect and optimize in the compiler. Either in a very hacky way\r\nof simple pattern matching or by better building blocks/optimizations in \r\nthe compiler.\r\n\r\nAfter all programming is a conversation with the compiler. If it doesn't \r\nget what you mean you can either ignore it and write assembler or teach it \r\nto understand you.\r\n\r\nBut I understand that there are currently implemented \"point in time \r\nsolutions\" here. \r\n\r\nI just sense a pattern here, that hand crafted assembler routines get \r\naccepted, while changes in the compiler to generate better code are neither \r\n(publically visible) done nor accepted.\r\n\r\nThis just feels wrong to me and I wondered whether this will change or is \r\nthe intended direction of development.\r\n\r\nBest Regards\r\n\r\nIngo Oeser\r\n",
			"disapproval": false,
			"date": "2012-08-24 13:32:33.677320",
			"approval": false
		},
		{
			"sender": "iant@golang.org",
			"recipients": [
				"gri@golang.org",
				"iant@golang.org",
				"remyoudompheng@gmail.com",
				"nightlyone@googlemail.com",
				"minux.ma@gmail.com",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "LGTM\n\nPlease feel free to either consider or ignore the various suggestions.\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s\nFile src/pkg/math/big/arith_amd64.s (right):\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode42\nsrc/pkg/math/big/arith_amd64.s:42: \nAt this point you could subtract 4 from DI.  That would save you the CMPQ in the loop, because you could branch back based on the SUBQ result.  Then you could add 4 back again before entering V1.\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode49\nsrc/pkg/math/big/arith_amd64.s:49: RCRQ $1, CX\t\t// restore CF\nDid you try moving the RCRQ instruction before the MOVQ instructions?\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode54\nsrc/pkg/math/big/arith_amd64.s:54: RCLQ $1, CX\t\t// save CF\nDid you try moving the RCLQ instruction after the MOVQ instructions?\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode55\nsrc/pkg/math/big/arith_amd64.s:55: MOVQ R11, 0(R10)(SI*8)\nDid you experiment with putting the MOVQ instruction immediately after the ADCQ instruction that generates it?  That would be worse on Atom but it might save a cycle on other x86_64 processors because it would avoid the immediate instruction dependency on the carry flag.  I don't know if it would help or hurt.\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode99\nsrc/pkg/math/big/arith_amd64.s:99: \nSame suggestion here about subtracting 4, of course.  And below, too.",
			"disapproval": false,
			"date": "2012-08-24 15:02:42.130550",
			"approval": true
		},
		{
			"sender": "nightlyone@googlemail.com",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Stupid question: Any reason this kind of low level optimization is not done \r\nat some peep-hole optimizer stage in the compiler via instruction pattern \r\nmatching?\r\n",
			"disapproval": false,
			"date": "2012-08-24 12:32:23.174390",
			"approval": false
		},
		{
			"sender": "iant@golang.org",
			"recipients": [
				"gri@golang.org",
				"iant@golang.org",
				"remyoudompheng@gmail.com",
				"nightlyone@googlemail.com",
				"minux.ma@gmail.com",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "It's difficult to use peephole optimizations to generate add-with-carry instructions.  The C or Go code for an add-with-carry instruction is fairly lengthy--try writing it yourself.  The resulting long code sequence is not a good fit for a peephole optimization.\n\nIt's somewhat more feasible to do it at a higher level in the compiler, but the 6g compiler is not really set up for higher level optimizations like this.\n\nI note that GCC is an aggressively optimizing compiler but it doesn't optimize add-with-carry sequences either.",
			"disapproval": false,
			"date": "2012-08-24 14:50:51.092200",
			"approval": false
		},
		{
			"sender": "gri@golang.org",
			"recipients": [
				"gri@golang.org",
				"iant@golang.org",
				"remyoudompheng@gmail.com",
				"nightlyone@googlemail.com",
				"minux.ma@gmail.com",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Thanks for the suggestions, Ian.\n\nJust for the record: I also wrote a version where at the end I have separate completely unrolled code for n = 1, 2, 3 (rather than the simple loop). This gives some modest speedup for short vectors but the amount of extra code and added irregularity seemed not worth it. I also tried moving that code before (rather than after the unrolled loop) because it permits the use of simpler (non-indexed) addressing modes for n = 1, 2, 3 - there was no difference in performance.\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s\nFile src/pkg/math/big/arith_amd64.s (right):\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode42\nsrc/pkg/math/big/arith_amd64.s:42: \nOn 2012/08/24 15:02:42, iant wrote:\n> At this point you could subtract 4 from DI.  That would save you the CMPQ in the\n> loop, because you could branch back based on the SUBQ result.  Then you could\n> add 4 back again before entering V1.\n\nYes, I was planning to do this but first wanted to get the loop right. Then I forgot to make the change. Will do in a separate CL.\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode49\nsrc/pkg/math/big/arith_amd64.s:49: RCRQ $1, CX\t\t// restore CF\nOn 2012/08/24 15:02:42, iant wrote:\n> Did you try moving the RCRQ instruction before the MOVQ instructions?\n\nI did in an earlier version and the results were inconclusive. Will try and perhaps produce a separate CL.\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode54\nsrc/pkg/math/big/arith_amd64.s:54: RCLQ $1, CX\t\t// save CF\nOn 2012/08/24 15:02:42, iant wrote:\n> Did you try moving the RCLQ instruction after the MOVQ instructions?\n\nsee above\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode55\nsrc/pkg/math/big/arith_amd64.s:55: MOVQ R11, 0(R10)(SI*8)\nOn 2012/08/24 15:02:42, iant wrote:\n> Did you experiment with putting the MOVQ instruction immediately after the ADCQ\n> instruction that generates it?  That would be worse on Atom but it might save a\n> cycle on other x86_64 processors because it would avoid the immediate\n> instruction dependency on the carry flag.  I don't know if it would help or\n> hurt.\n\nI have not. Will try all these - it's easy enough.\n\nhttp://codereview.appspot.com/6482062/diff/6002/src/pkg/math/big/arith_amd64.s#newcode99\nsrc/pkg/math/big/arith_amd64.s:99: \nOn 2012/08/24 15:02:42, iant wrote:\n> Same suggestion here about subtracting 4, of course.  And below, too.\n\nDone.",
			"disapproval": false,
			"date": "2012-08-24 16:08:53.693230",
			"approval": false
		},
		{
			"sender": "remyoudompheng@gmail.com",
			"recipients": [
				"gri@golang.org",
				"iant@golang.org",
				"remyoudompheng@gmail.com",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Looks nice, thanks!",
			"disapproval": false,
			"date": "2012-08-24 06:43:50.306500",
			"approval": false
		},
		{
			"sender": "gri@golang.org",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "I have proposed in the past the use of the comma-ok form in Go for getting\r\ncarry bit information. For instance, one could imagine writing\r\n\r\n    sum, carry = x + y\r\n\r\nwhere carry is 0 or 1. With this one might write\r\n\r\n    func addVV(z, x, y []Word) Word\r\n        carry := 0\r\n        for i := range z {\r\n            z[i], carry = x[i] + y[i] + carry\r\n        }\r\n        return carry\r\n    }\r\n\r\nwhich corresponds to the addVV assembly routine. One would also have to\r\ntell the compiler to not do any array bounds checks. It's conceivable that\r\neven a fairly simple compiler could generate rather good for this. A better\r\ncompiler might even do the unrolling. That said, it's a lot of stuff that\r\nneeds to be just right and at that point it's still not clear that a\r\ncompiler can beat a hand-tuned version. The reason for the assembly\r\nroutines in the first place is best-possible performance - they are not\r\nreally needed for functionality since there are Go versions of all these\r\nassembly routines.\r\n\r\n- gri\r\n\r\n\r\nOn Fri, Aug 24, 2012 at 6:32 AM, Ingo Oeser <nightlyone@googlemail.com>wrote:\r\n\r\n> On Friday, August 24, 2012 3:06:57 PM UTC+2, minux wrote:\r\n>\r\n>>\r\n>> On Fri, Aug 24, 2012 at 8:32 PM, Ingo Oeser <night...@googlemail.com>wrote:\r\n>>\r\n>>> Stupid question: Any reason this kind of low level optimization is not\r\n>>> done at some peep-hole optimizer stage in the compiler via instruction\r\n>>> pattern matching?\r\n>>>\r\n>> you mean unrolling loops written in assembly programs?\r\n>\r\n>\r\n> That would be a linker stage but would require code size and code speed\r\n> metrics to find a tradeoff.\r\n>\r\n>\r\n>> as we don't have much assembly routines, adding such a optimizing pass\r\n>> doesn't\r\n>> seem worthy (beside this pass will only have very limited applicability).\r\n>>\r\n>>\r\n> The idea was: We add highly optimized assembler routines for things we\r\n> could detect and optimize in the compiler. Either in a very hacky way\r\n> of simple pattern matching or by better building blocks/optimizations in\r\n> the compiler.\r\n>\r\n> After all programming is a conversation with the compiler. If it doesn't\r\n> get what you mean you can either ignore it and write assembler or teach it\r\n> to understand you.\r\n>\r\n> But I understand that there are currently implemented \"point in time\r\n> solutions\" here.\r\n>\r\n> I just sense a pattern here, that hand crafted assembler routines get\r\n> accepted, while changes in the compiler to generate better code are neither\r\n> (publically visible) done nor accepted.\r\n>\r\n> This just feels wrong to me and I wondered whether this will change or is\r\n> the intended direction of development.\r\n>\r\n> Best Regards\r\n>\r\n> Ingo Oeser\r\n>\r\n",
			"disapproval": false,
			"date": "2012-08-24 17:14:23.310320",
			"approval": false
		}
	],
	"owner_email": "gri@golang.org",
	"private": false,
	"base_url": "",
	"owner": "gri",
	"subject": "code review 6482062: math/big: faster (add|sub)V(V|W) routines",
	"created": "2012-08-24 06:05:49.325260",
	"patchsets": [
		1,
		2001,
		3002,
		3003,
		6002,
		11001
	],
	"modified": "2012-08-24 16:21:09.073770",
	"closed": true,
	"issue": 6482062
}