{
	"description": "encoding/binary: Reduce memory allocations and add fast paths for slices\n\nAlso added a new test and a few benchmarks for different basic slice types.\n\nFixes issue 2634\n\nbenchmark                         old ns/op    new ns/op    delta\nBenchmarkReadSlice1000Int8s           35700         3328  -90.68%\nBenchmarkReadSlice1000Uint8s          34159          110  -99.68%\nBenchmarkReadSlice1000Int16s          46197        14607  -68.38%\nBenchmarkReadSlice1000Uint16s         44314        14693  -66.84%\nBenchmarkReadSlice1000Int32s          49191        19269  -60.83%\nBenchmarkReadSlice1000Uint32s         47523        18213  -61.68%\nBenchmarkReadSlice1000Int64s          61243        29637  -51.61%\nBenchmarkReadSlice1000Uint64s         57205        27828  -51.35%\nBenchmarkWriteSlice1000Int8s          40406         2217  -94.51%\nBenchmarkWriteSlice1000Uint8s         40316           99  -99.75%\nBenchmarkWriteSlice1000Int16s         51168        11912  -76.72%\nBenchmarkWriteSlice1000Uint16s        49556        11840  -76.11%\nBenchmarkWriteSlice1000Int32s         54567        15955  -70.76%\nBenchmarkWriteSlice1000Uint32s        51732        15800  -69.46%\nBenchmarkWriteSlice1000Int64s         67872        24318  -64.17%\nBenchmarkWriteSlice1000Uint64s        61171        23469  -61.63%\n\nbenchmark                          old MB/s     new MB/s  speedup\nBenchmarkReadSlice1000Int8s           28.01       300.41   10.73x\nBenchmarkReadSlice1000Uint8s          29.27      9051.21  309.23x\nBenchmarkReadSlice1000Int16s          43.29       136.92    3.16x\nBenchmarkReadSlice1000Uint16s         45.13       136.11    3.02x\nBenchmarkReadSlice1000Int32s          81.31       207.58    2.55x\nBenchmarkReadSlice1000Uint32s         84.17       219.61    2.61x\nBenchmarkReadSlice1000Int64s         130.63       269.93    2.07x\nBenchmarkReadSlice1000Uint64s        139.85       287.48    2.06x\nBenchmarkWriteSlice1000Int8s          24.75       451.01   18.22x\nBenchmarkWriteSlice1000Uint8s         24.80     10104.20  407.43x\nBenchmarkWriteSlice1000Int16s         39.09       167.89    4.29x\nBenchmarkWriteSlice1000Uint16s        40.36       168.91    4.19x\nBenchmarkWriteSlice1000Int32s         73.30       250.70    3.42x\nBenchmarkWriteSlice1000Uint32s        77.32       253.16    3.27x\nBenchmarkWriteSlice1000Int64s        117.87       328.97    2.79x\nBenchmarkWriteSlice1000Uint64s       130.78       340.87    2.61x\n\nbenchmark                        old allocs   new allocs    delta\nBenchmarkReadSlice1000Int8s               2            1  -50.00%\nBenchmarkReadSlice1000Uint8s              2            1  -50.00%\nBenchmarkReadSlice1000Int16s              2            1  -50.00%\nBenchmarkReadSlice1000Uint16s             2            1  -50.00%\nBenchmarkReadSlice1000Int32s              2            1  -50.00%\nBenchmarkReadSlice1000Uint32s             2            1  -50.00%\nBenchmarkReadSlice1000Int64s              2            1  -50.00%\nBenchmarkReadSlice1000Uint64s             2            1  -50.00%\nBenchmarkWriteSlice1000Int8s              3            1  -66.67%\nBenchmarkWriteSlice1000Uint8s             3            1  -66.67%\nBenchmarkWriteSlice1000Int16s             3            1  -66.67%\nBenchmarkWriteSlice1000Uint16s            3            1  -66.67%\nBenchmarkWriteSlice1000Int32s             3            1  -66.67%\nBenchmarkWriteSlice1000Uint32s            3            1  -66.67%\nBenchmarkWriteSlice1000Int64s             3            1  -66.67%\nBenchmarkWriteSlice1000Uint64s            3            1  -66.67%\n\nbenchmark                         old bytes    new bytes    delta\nBenchmarkReadSlice1000Int8s            1072            8  -99.25%\nBenchmarkReadSlice1000Uint8s           1072            8  -99.25%\nBenchmarkReadSlice1000Int16s           2096            8  -99.62%\nBenchmarkReadSlice1000Uint16s          2096            8  -99.62%\nBenchmarkReadSlice1000Int32s           4144            8  -99.81%\nBenchmarkReadSlice1000Uint32s          4144            8  -99.81%\nBenchmarkReadSlice1000Int64s           8240            8  -99.90%\nBenchmarkReadSlice1000Uint64s          8240            8  -99.90%\nBenchmarkWriteSlice1000Int8s           1080            8  -99.26%\nBenchmarkWriteSlice1000Uint8s          1080            8  -99.26%\nBenchmarkWriteSlice1000Int16s          2104            8  -99.62%\nBenchmarkWriteSlice1000Uint16s         2104            8  -99.62%\nBenchmarkWriteSlice1000Int32s          4152            8  -99.81%\nBenchmarkWriteSlice1000Uint32s         4152            8  -99.81%\nBenchmarkWriteSlice1000Int64s          8248            8  -99.90%\nBenchmarkWriteSlice1000Uint64s         8248            8  -99.90%\n",
	"cc": [
		"golang-dev@googlegroups.com"
	],
	"reviewers": [
		"golang-dev@googlegroups.com",
		"r@golang.org",
		"iant@golang.org",
		"rsc@golang.org"
	],
	"messages": [
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Hello golang-dev@googlegroups.com,\n\nI'd like you to review this change to\nhttps://code.google.com/p/go",
			"disapproval": false,
			"date": "2013-08-06 10:24:39.880530",
			"approval": false
		},
		{
			"sender": "r@golang.org",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "This is too complicated and pulls in too much. All that's required is a few more cases in the big switch in Write.\n\nhttps://codereview.appspot.com/12521043/diff/6001/src/pkg/encoding/binary/binary.go\nFile src/pkg/encoding/binary/binary.go (right):\n\nhttps://codereview.appspot.com/12521043/diff/6001/src/pkg/encoding/binary/binary.go#newcode316\nsrc/pkg/encoding/binary/binary.go:316: if r := recover(); r != nil {\ndon't do this\n\nhttps://codereview.appspot.com/12521043/diff/6001/src/pkg/encoding/binary/binary.go#newcode435\nsrc/pkg/encoding/binary/binary.go:435: // Fast path for common slice types\nYou don't need reflection to do these cases. You've changed more than is necessary and sped it up less than is possible.",
			"disapproval": false,
			"date": "2013-08-06 10:46:00.209240",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On 2013/08/06 10:46:00, r wrote:\n> This is too complicated and pulls in too much. All that's required is a few more\n> cases in the big switch in Write.\n> \n> https://codereview.appspot.com/12521043/diff/6001/src/pkg/encoding/binary/binary.go\n> File src/pkg/encoding/binary/binary.go (right):\n> \n> https://codereview.appspot.com/12521043/diff/6001/src/pkg/encoding/binary/binary.go#newcode316\n> src/pkg/encoding/binary/binary.go:316: if r := recover(); r != nil {\n> don't do this\n\nencoding/json does exactly the same and it saves a lot of if err != nil checks.\n\n> \n> https://codereview.appspot.com/12521043/diff/6001/src/pkg/encoding/binary/binary.go#newcode435\n> src/pkg/encoding/binary/binary.go:435: // Fast path for common slice types\n> You don't need reflection to do these cases. You've changed more than is\n> necessary and sped it up less than is possible.\n\nThat requires one of the following:\n\n1 - Always initialize the encoder/decoder, which is not required for basic types (degrades performance for basic types)\n2 - Copy and paste the line for initializing the encoder/decoder in every type switch case for slices (ugly and prone to errors).\n\nI had already tested option #2 and, guess what? It's slower than this patch. Probably because of the way Go handles switches (disassembling the code and finding exactly why is left as an exercise for the reader). Left is this CL, right is the code you suggest:\n\nbenchmark                        old ns/op    new ns/op    delta\nBenchmarkWriteSlice1000Int32s        25962        27278   +5.07%\n\nbenchmark                         old MB/s     new MB/s  speedup\nBenchmarkWriteSlice1000Int32s       154.07       146.63    0.95x\n\nbenchmark                       old allocs   new allocs    delta\nBenchmarkWriteSlice1000Int32s            4            3  -25.00%\n\nbenchmark                        old bytes    new bytes    delta\nBenchmarkWriteSlice1000Int32s          140          105  -25.00%\n\nOne could argue that saving 35 bytes per run is worth the 5% performance penalty, but I don't think that's the case. Note that I didn't include error checking in this test, so the final version would be even slower. Furthermore, this CL will also speed up any struct which contains slices, while the code you suggest won't.\n\nRegards,\nAlberto\n\nP.S.: Your suggested patch follows, I've omitted error checking in the big switch in Write() for slices, so the final version would be even slower:\n\nfiam@ubuntu:~/go/src/pkg/encoding/binary$ cat r.diff \ndiff -r ab644299d124 src/pkg/encoding/binary/binary.go\n--- a/src/pkg/encoding/binary/binary.go\tMon Aug 05 15:46:06 2013 -0700\n+++ b/src/pkg/encoding/binary/binary.go\tTue Aug 06 13:44:30 2013 +0200\n@@ -20,6 +20,7 @@\n \t\"io\"\n \t\"math\"\n \t\"reflect\"\n+\t\"runtime\"\n )\n \n // A ByteOrder specifies how to convert byte sequences into\n@@ -167,14 +168,10 @@\n \tdefault:\n \t\treturn errors.New(\"binary.Read: invalid type \" + d.Type().String())\n \t}\n-\tsize, err := dataSize(v)\n-\tif err != nil {\n+\tif _, err := dataSize(v); err != nil {\n \t\treturn errors.New(\"binary.Read: \" + err.Error())\n \t}\n-\td := &decoder{order: order, buf: make([]byte, size)}\n-\tif _, err := io.ReadFull(r, d.buf); err != nil {\n-\t\treturn err\n-\t}\n+\td := decoder{coder: coder{order: order}, reader: r}\n \td.value(v)\n \treturn nil\n }\n@@ -239,6 +236,54 @@\n \tcase uint64:\n \t\tbs = b[:8]\n \t\torder.PutUint64(bs, v)\n+\tcase []uint8:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.uint8(val)\n+\t\t}\n+\t\treturn e.err\n+\tcase []int8:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.int8(val)\n+\t\t}\n+\t\treturn e.err\n+\tcase []uint16:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.uint16(val)\n+\t\t}\n+\t\treturn e.err\n+\tcase []int16:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.int16(val)\n+\t\t}\n+\t\treturn e.err\n+\tcase []uint32:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.uint32(val)\n+\t\t}\n+\t\treturn e.err\n+\tcase []int32:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.int32(val)\n+\t\t}\n+\t\treturn e.err\n+\tcase []uint64:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.uint64(val)\n+\t\t}\n+\t\treturn e.err\n+\tcase []int64:\n+\t\te := encoder{coder: coder{order: order}, writer: w}\n+\t\tfor _, val := range v {\n+\t\t\te.int64(val)\n+\t\t}\n+\t\treturn e.err\n \t}\n \tif bs != nil {\n \t\t_, err := w.Write(bs)\n@@ -247,15 +292,13 @@\n \n \t// Fallback to reflect-based encoding.\n \tv := reflect.Indirect(reflect.ValueOf(data))\n-\tsize, err := dataSize(v)\n-\tif err != nil {\n+\tif _, err := dataSize(v); err != nil {\n \t\treturn errors.New(\"binary.Write: \" + err.Error())\n \t}\n-\tbuf := make([]byte, size)\n-\te := &encoder{order: order, buf: buf}\n+\te := encoder{coder: coder{order: order}, writer: w}\n+\tdefer e.recover()\n \te.value(v)\n-\t_, err = w.Write(buf)\n-\treturn err\n+\treturn e.err\n }\n \n // Size returns how many bytes Write would generate to encode the value v, which\n@@ -313,54 +356,87 @@\n \n type coder struct {\n \torder ByteOrder\n-\tbuf   []byte\n+\tbuf   [8]byte\n+\terr   error\n }\n \n-type decoder coder\n-type encoder coder\n+func (c *coder) recover() {\n+\tif r := recover(); r != nil {\n+\t\tif _, ok := r.(runtime.Error); ok {\n+\t\t\tpanic(r)\n+\t\t}\n+\t\tc.err = r.(error)\n+\t}\n+}\n+\n+type decoder struct {\n+\tcoder\n+\treader io.Reader\n+}\n+\n+func (d *decoder) read(bs []byte) {\n+\tif _, err := d.reader.Read(bs); err != nil {\n+\t\tpanic(err)\n+\t}\n+}\n+\n+type encoder struct {\n+\tcoder\n+\twriter io.Writer\n+}\n+\n+func (e *encoder) write(bs []byte) {\n+\tif _, err := e.writer.Write(bs); err != nil {\n+\t\tpanic(err)\n+\t}\n+}\n \n func (d *decoder) uint8() uint8 {\n-\tx := d.buf[0]\n-\td.buf = d.buf[1:]\n-\treturn x\n+\tbs := d.buf[:1]\n+\td.read(bs)\n+\treturn bs[0]\n }\n \n func (e *encoder) uint8(x uint8) {\n-\te.buf[0] = x\n-\te.buf = e.buf[1:]\n+\tbs := e.buf[:1]\n+\tbs[0] = x\n+\te.write(bs)\n }\n \n func (d *decoder) uint16() uint16 {\n-\tx := d.order.Uint16(d.buf[0:2])\n-\td.buf = d.buf[2:]\n-\treturn x\n+\tbs := d.buf[:2]\n+\td.read(bs)\n+\treturn d.order.Uint16(bs)\n }\n \n func (e *encoder) uint16(x uint16) {\n-\te.order.PutUint16(e.buf[0:2], x)\n-\te.buf = e.buf[2:]\n+\tbs := e.buf[:2]\n+\te.order.PutUint16(bs, x)\n+\te.write(bs)\n }\n \n func (d *decoder) uint32() uint32 {\n-\tx := d.order.Uint32(d.buf[0:4])\n-\td.buf = d.buf[4:]\n-\treturn x\n+\tbs := d.buf[:4]\n+\td.read(bs)\n+\treturn d.order.Uint32(bs)\n }\n \n func (e *encoder) uint32(x uint32) {\n-\te.order.PutUint32(e.buf[0:4], x)\n-\te.buf = e.buf[4:]\n+\tbs := e.buf[:4]\n+\te.order.PutUint32(bs, x)\n+\te.write(bs)\n }\n \n func (d *decoder) uint64() uint64 {\n-\tx := d.order.Uint64(d.buf[0:8])\n-\td.buf = d.buf[8:]\n-\treturn x\n+\tbs := d.buf[:8]\n+\td.read(bs)\n+\treturn d.order.Uint64(bs)\n }\n \n func (e *encoder) uint64(x uint64) {\n-\te.order.PutUint64(e.buf[0:8], x)\n-\te.buf = e.buf[8:]\n+\tbs := e.buf[:8]\n+\te.order.PutUint64(bs, x)\n+\te.write(bs)\n }\n \n func (d *decoder) int8() int8 { return int8(d.uint8()) }\n@@ -404,9 +480,45 @@\n \t\t}\n \n \tcase reflect.Slice:\n-\t\tl := v.Len()\n-\t\tfor i := 0; i < l; i++ {\n-\t\t\td.value(v.Index(i))\n+\t\t// Fast path for common slice types\n+\t\tswitch s := v.Interface().(type) {\n+\t\tcase []uint8:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.uint8()\n+\t\t\t}\n+\t\tcase []int8:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.int8()\n+\t\t\t}\n+\t\tcase []uint16:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.uint16()\n+\t\t\t}\n+\t\tcase []int16:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.int16()\n+\t\t\t}\n+\t\tcase []uint32:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.uint32()\n+\t\t\t}\n+\t\tcase []int32:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.int32()\n+\t\t\t}\n+\t\tcase []uint64:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.uint64()\n+\t\t\t}\n+\t\tcase []int64:\n+\t\t\tfor i := range s {\n+\t\t\t\ts[i] = d.int64()\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tl := v.Len()\n+\t\t\tfor i := 0; i < l; i++ {\n+\t\t\t\td.value(v.Index(i))\n+\t\t\t}\n \t\t}\n \n \tcase reflect.Int8:\n@@ -466,9 +578,45 @@\n \t\t}\n \n \tcase reflect.Slice:\n-\t\tl := v.Len()\n-\t\tfor i := 0; i < l; i++ {\n-\t\t\te.value(v.Index(i))\n+\t\t// Fast path for common slice types\n+\t\tswitch s := v.Interface().(type) {\n+\t\tcase []uint8:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.uint8(val)\n+\t\t\t}\n+\t\tcase []int8:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.int8(val)\n+\t\t\t}\n+\t\tcase []uint16:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.uint16(val)\n+\t\t\t}\n+\t\tcase []int16:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.int16(val)\n+\t\t\t}\n+\t\tcase []uint32:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.uint32(val)\n+\t\t\t}\n+\t\tcase []int32:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.int32(val)\n+\t\t\t}\n+\t\tcase []uint64:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.uint64(val)\n+\t\t\t}\n+\t\tcase []int64:\n+\t\t\tfor _, val := range s {\n+\t\t\t\te.int64(val)\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tl := v.Len()\n+\t\t\tfor i := 0; i < l; i++ {\n+\t\t\t\te.value(v.Index(i))\n+\t\t\t}\n \t\t}\n \n \tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n@@ -519,15 +667,29 @@\n \n func (d *decoder) skip(v reflect.Value) {\n \tn, _ := dataSize(v)\n-\td.buf = d.buf[n:]\n+\tb := d.buf[:8]\n+\tfor n >= 8 {\n+\t\td.read(b)\n+\t\tn -= 8\n+\t}\n+\tif n > 0 {\n+\t\td.read(b[:n])\n+\t}\n }\n \n func (e *encoder) skip(v reflect.Value) {\n \tn, _ := dataSize(v)\n-\tfor i := range e.buf[0:n] {\n-\t\te.buf[i] = 0\n+\tfor ii := 0; ii < 8; ii++ {\n+\t\te.buf[ii] = 0\n \t}\n-\te.buf = e.buf[n:]\n+\tb := e.buf[:8]\n+\tfor n >= 8 {\n+\t\te.write(b)\n+\t\tn -= 8\n+\t}\n+\tif n > 0 {\n+\t\te.write(b[:n])\n+\t}\n }\n \n // intDestSize returns the size of the integer that ptrType points to,",
			"disapproval": false,
			"date": "2013-08-06 11:57:29.349200",
			"approval": false
		},
		{
			"sender": "r@golang.org",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "I think one of us isn't listening. It may be me, but I was expecting to see\r\nWrite look like this, to pick one snippet:\r\n\r\nWe now have\r\n\r\ncase uint8:\r\n bs = b[:1]\r\nb[0] = byte(v)\r\n\r\n\r\nafter that something like\r\n\r\n case []uint8:\r\nif len(v) > len(bs) { // This check wouldn't be worth the bother for types\r\nlarge than 8 bits.\r\n bs = make([]byte, len(v))\r\n} else {\r\nbs = b[:len(v)]\r\n }\r\ncopy(bs, v)\r\n\r\nThere is no need for reflection or encoders for slices of basic types. Just\r\nstuff the bits.\r\n\r\n-rob\r\n",
			"disapproval": false,
			"date": "2013-08-06 12:13:39.899870",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On 2013/08/06 12:13:39, r wrote:\n> I think one of us isn't listening. It may be me, but I was expecting to see\n> Write look like this, to pick one snippet:\n> \n> We now have\n> \n> case uint8:\n>  bs = b[:1]\n> b[0] = byte(v)\n> \n> \n> after that something like\n> \n>  case []uint8:\n> if len(v) > len(bs) { // This check wouldn't be worth the bother for types\n> large than 8 bits.\n>  bs = make([]byte, len(v))\n> } else {\n> bs = b[:len(v)]\n>  }\n> copy(bs, v)\n> \n> There is no need for reflection or encoders for slices of basic types. Just\n> stuff the bits.\n> \n> -rob\n\nThat would allocate garbage and run slower than the current patch set. However, you've just given me an idea to make it even faster. I have to run some errands right now, but I'll update this CL tonight with a better patch set.\n\nRegards,\nAlberto",
			"disapproval": false,
			"date": "2013-08-06 14:51:51.190810",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Hello golang-dev@googlegroups.com, r@golang.org (cc: golang-dev@googlegroups.com),\n\nPlease take another look.",
			"disapproval": false,
			"date": "2013-08-06 23:28:11.969700",
			"approval": false
		},
		{
			"sender": "r@golang.org",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "I apologize but your goals for this package seem to be different from ours. You've doubled the size of the source code, added panics and recovers, even unrolled loops by hand. This package doesn't deserve that level of attention for performance. Anyone who *needs* that much performance is better off doing the byte stuffing themselves, since this package is both severely limited in power and unable to be performant even in all the cases it does handle.\n\nI have several times suggested a much simpler approach that will make writing slices significantly faster in a way that seems more in keeping with the modest goals of this package, but you are clearly not satisfied by that approach.  So although I appreciate the work you've done, perhaps your code should be published in a separate open source repository.\n\nI will update the package documentation to make its limitations clearer.",
			"disapproval": false,
			"date": "2013-08-09 06:17:50.757310",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On 2013/08/09 06:17:50, r wrote:\n> I apologize but your goals for this package seem to be different from ours.\n> You've doubled the size of the source code, added panics and recovers, even\n> unrolled loops by hand. This package doesn't deserve that level of attention for\n> performance. Anyone who *needs* that much performance is better off doing the\n> byte stuffing themselves, since this package is both severely limited in power\n> and unable to be performant even in all the cases it does handle.\n> \n> I have several times suggested a much simpler approach that will make writing\n> slices significantly faster in a way that seems more in keeping with the modest\n> goals of this package, but you are clearly not satisfied by that approach.  So\n> although I appreciate the work you've done, perhaps your code should be\n> published in a separate open source repository.\n> \n> I will update the package documentation to make its limitations clearer.\n\nI respectfully disagree with that. With a few more optimizations (like writing ByteOrder methods in assembly), this package would be more than fast enough for almost everyone. And that's an optimization that most users can't perform themselves (specially when using App Engine, since AFAIK user code most be pure Go on GAE, but the standard library can use ASM and unsafe - please correct me on that if I'm wrong). Also, performance increases are nice for everybody, even when they don't absolutely need them. If your CPU spends less time in encoding/binary, you have more time to do other stuff. Of course, performance improvements usually come with a maintenance cost, because most of the time you're making the code more complex.  However, I would consider encoding/binary feature complete so I think making the code a bit more complex to make it more performant is, at this time, acceptable.\n\nRemember when people used to say that it didn't matter that Python/Ruby web frameworks were slow because the bottleneck was gonna be the database anyway? Well, Go has proved all them wrong because websites that migrated their code to Go have been able to significantly reduce their number of servers (some of us were already doing that back in the day rewriting the hottest code paths in C and ASM, but it's nice we don't have do that anymore thanks to Go).\n\nOn the other hand, the approach you suggested would be indeed, simpler, but also completely wrong because it would allocate unnecessary garbage which is going to be throw again before the function ends.\n\nAnyway, if your stance on performance is that users should rewrite parts of the standard library when they need fast code, I will close this CL and stop submitting patches which just increase performance (and, please, close the issues related to performance with a WONTFIX, so everybody gets the message).\n\nRegards,\nAlberto",
			"disapproval": false,
			"date": "2013-08-09 10:04:50.293890",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "*** Abandoned ***",
			"disapproval": false,
			"date": "2013-08-09 10:16:43.118580",
			"approval": false
		},
		{
			"sender": "iant@golang.org",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On Fri, Aug 9, 2013 at 3:04 AM,  <alberto.garcia.hierro@gmail.com> wrote:\r\n>\r\n> Anyway, if your stance on performance is that users should rewrite parts\r\n> of the standard library when they need fast code, I will close this CL\r\n> and stop submitting patches which just increase performance (and,\r\n> please, close the issues related to performance with a WONTFIX, so\r\n> everybody gets the message).\r\n\r\nI want to point out that Rob was clear that he was talking about a\r\nspecific package, binary/encoding, and in particular argued that the\r\npackage, by design, can not be as fast as code that does its own byte\r\nstuffing.  Your extension to all performance issues in the standard\r\nlibrary is understandable but erroneous.\r\n\r\nIan\r\n",
			"disapproval": false,
			"date": "2013-08-09 13:13:52.586970",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"iant@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On 2013/08/09 13:13:52, iant wrote:\n> On Fri, Aug 9, 2013 at 3:04 AM,  <mailto:alberto.garcia.hierro@gmail.com> wrote:\n> >\n> > Anyway, if your stance on performance is that users should rewrite parts\n> > of the standard library when they need fast code, I will close this CL\n> > and stop submitting patches which just increase performance (and,\n> > please, close the issues related to performance with a WONTFIX, so\n> > everybody gets the message).\n> \n> I want to point out that Rob was clear that he was talking about a\n> specific package, binary/encoding, and in particular argued that the\n> package, by design, can not be as fast as code that does its own byte\n> stuffing.  Your extension to all performance issues in the standard\n> library is understandable but erroneous.\n> \n> Ian\n\nI wasn't stating that he was talking about the whole standard library, but rather posing the question. That's why I started the phrase with \"if your stance on performance\u2026\".\n\nRegards,\nAlberto",
			"disapproval": false,
			"date": "2013-08-09 16:59:20.489070",
			"approval": false
		},
		{
			"sender": "iant@golang.org",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On Fri, Aug 9, 2013 at 9:59 AM,  <alberto.garcia.hierro@gmail.com> wrote:\r\n> On 2013/08/09 13:13:52, iant wrote:\r\n>\r\n>> I want to point out that Rob was clear that he was talking about a\r\n>> specific package, binary/encoding, and in particular argued that the\r\n>> package, by design, can not be as fast as code that does its own byte\r\n>> stuffing.  Your extension to all performance issues in the standard\r\n>> library is understandable but erroneous.\r\n>\r\n>\r\n>> Ian\r\n>\r\n>\r\n> I wasn't stating that he was talking about the whole standard library,\r\n> but rather posing the question. That's why I started the phrase with \"if\r\n> your stance on performance\u2026\".\r\n\r\nMy apologies for misunderstanding.\r\n\r\nI can't speak for Rob but I don't think most Go developers would agree\r\nwith the general notion that \"users should rewrite parts of the\r\nstandard library when they need fast code.\"  So if you are asking that\r\nas a question, the answer is no.\r\n\r\nIan\r\n",
			"disapproval": false,
			"date": "2013-08-09 17:29:37.776300",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"iant@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On 2013/08/09 17:29:37, iant wrote:\n> On Fri, Aug 9, 2013 at 9:59 AM,  <mailto:alberto.garcia.hierro@gmail.com> wrote:\n> > On 2013/08/09 13:13:52, iant wrote:\n> >\n> >> I want to point out that Rob was clear that he was talking about a\n> >> specific package, binary/encoding, and in particular argued that the\n> >> package, by design, can not be as fast as code that does its own byte\n> >> stuffing.  Your extension to all performance issues in the standard\n> >> library is understandable but erroneous.\n> >\n> >\n> >> Ian\n> >\n> >\n> > I wasn't stating that he was talking about the whole standard library,\n> > but rather posing the question. That's why I started the phrase with \"if\n> > your stance on performance\u2026\".\n> \n> My apologies for misunderstanding.\n> \n> I can't speak for Rob but I don't think most Go developers would agree\n> with the general notion that \"users should rewrite parts of the\n> standard library when they need fast code.\"  So if you are asking that\n> as a question, the answer is no.\n> \n> Ian\n\nThanks for the clarification. Now that I re-read it, maybe the question wasn't clear enough (English is my third language and I mix things up from time to time).\n\nRegards,\nAlberto",
			"disapproval": false,
			"date": "2013-08-09 18:31:25.861960",
			"approval": false
		},
		{
			"sender": "rsc@golang.org",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Alberto,\r\n\r\nYou seem singularly focused on performance, willing to go to great lengths\r\nto get a couple percent speedup. What you seem not to appreciate is the\r\nweight we assign to other valid concerns, such as code complexity,\r\nmaintainability, and reliability. For those, we are often willing to give\r\nup a few percent or more. Code that is left unchanged is code that is more\r\nlikely to be correct, and code that is kept simple is code that will be\r\neasier to adjust later. Rewriting a large amount of code to achieve a\r\ncertain speed improvement is almost never better than rewriting a much\r\nsmaller amount of code to achieve an only slightly smaller improvement.\r\nFocus on the big savings and the tiny things will take care of themselves.\r\nYour original CL made BenchmarkWriteSlice1000Int32s 2.15x faster. The\r\nsmaller, simpler CL that Rob submitted made it 2.14x faster. What's left in\r\nyour CL would be another 1.2x if I am reading the numbers correct, and 1.2x\r\ndoesn't really pay for such a large change.\r\n\r\nYour comparison with Python and Ruby is not really analogous to what's\r\ngoing on here. Python and Ruby are not just 1.2x or even 2.5x slower than\r\ncompiled code.\r\n\r\nRuss\r\n",
			"disapproval": false,
			"date": "2013-08-09 18:33:55.344100",
			"approval": false
		},
		{
			"sender": "alberto.garcia.hierro@gmail.com",
			"recipients": [
				"alberto.garcia.hierro@gmail.com",
				"golang-dev@googlegroups.com",
				"r@golang.org",
				"iant@golang.org",
				"rsc@golang.org",
				"golang-dev@googlegroups.com",
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "On 2013/08/09 18:33:55, rsc wrote:\nHi Russ,\n\nMy focus for this package is indeed performance, because I'd really love it to be fast enough to not need to write my own byte stuffing routines most of time. Or, at least, leverage encoding/binary to do so. I understand that maintainability of the code is also a concern, but I don't think the code that I wrote is inherently more difficult to maintain than it was before. Unrolled loops are ugly, that's for certain, but I find them an acceptable tradeoff in this case, mostly because the compiler will never be able to perform those for me. As for the reliability, my code passes all the tests in encoding/binary and few more than I added, so I'm pretty confident about its correctness. \n\nI'd like to clarify that this CL was an initial patch set, which I considered the bare minimum. I planned to continue improving the performance of the package with more CLs, as well as cleaning up the code a bit and separating its code into several better organized files. The reason I didn't wait until I had all those changes ready is because the other day (on the encoding/json CL) I was told to submit small changes, not big ones, because is difficult to measure each individual change if they're presented as a single one.\n\nWith this package I took the approach to send a small change first, just to be told that what I was doing could (wrongly) be done with less code. Although my initial patch was just 20% faster than Rob's code, it dramatically reduced the amount of generated garbage. When it comes to bytes allocated, Rob's code is O(N) while mine is O(1) and uses a very small amount (usually, 8 bytes) per run. Reducing the pressure on the GC benefits real world applications a lot, but this was just ignored by Rob.\n\nOn the other hand, the code in ByteOrder implementations is less than suboptimal and, at the same time, probably one of the best candidates to be rewritten in assembler, because the gc compiler won't generate BSWAP instructions. Having an optimal ByteOrder implementation is a big win for users, because they can leverage their performance to write their own specialized byte stuffing routines for their own types. And there's no way to get that optimal performance without resorting to assembler, which I wouldn't expect Go users to do.\n\nWith that said, I spent a couple more of hours optimizing my fork of encoding/binary and got pretty good results (at least for my criteria):\n\nbenchmark                          old ns/op    new ns/op    delta\nBenchmarkReadSlice1000Int8s             2992         2909   -2.77%\nBenchmarkReadSlice1000Uint8s            3151           32  -98.98%\nBenchmarkReadSlice1000Int16s           19003        10529  -44.59%\nBenchmarkReadSlice1000Uint16s          13451        11669  -13.25%\nBenchmarkReadSlice1000Int32s           19568        13510  -30.96%\nBenchmarkReadSlice1000Uint32s          27385        13323  -51.35%\nBenchmarkReadSlice1000Int64s           42735        13343  -68.78%\nBenchmarkReadSlice1000Uint64s          31739        13850  -56.36%\nBenchmarkWriteSlice1000Int8s            3224         2271  -29.56%\nBenchmarkWriteSlice1000Uint8s            108           29  -72.69%\nBenchmarkWriteSlice1000Int16s          12866         7950  -38.21%\nBenchmarkWriteSlice1000Uint16s         17632         9666  -45.18%\nBenchmarkWriteSlice1000Int32s          21643        10436  -51.78%\nBenchmarkWriteSlice1000Uint32s         23070        10508  -54.45%\nBenchmarkWriteSlice1000Int64s          32699        10520  -67.83%\nBenchmarkWriteSlice1000Uint64s         31710        10559  -66.70%\nBenchmarkReadStruct                     3569         2035  -42.98%\nBenchmarkWriteStruct                    3705         1773  -52.15%\nBenchmarkReadStructCustom                879          763  -13.20%\nBenchmarkWriteStructCustom               288          217  -24.65%\nBenchmarkReadSliceStruct                 726        27079  +3629.89%\nBenchmarkReadSliceStructCustom         55581        44792  -19.41%\nBenchmarkWriteSliceStructCustom        19856         9226  -53.54%\nBenchmarkReadArrayStruct               61334        26983  -56.01%\nBenchmarkWriteArrayStruct              64916        13945  -78.52%\nBenchmarkReadInts                       1307         1198   -8.34%\nBenchmarkWriteInts                      1262         1369   +8.48%\nBenchmarkPutBigEndian                     29           12  -58.72%\nBenchmarkPutLittleEndian                  27           11  -58.06%\nBenchmarkReadBigEndian                    28           12  -58.04%\nBenchmarkReadLittleEndian                 28           11  -58.72%\n\nbenchmark                           old MB/s     new MB/s  speedup\nBenchmarkReadSlice1000Int8s           334.13       343.65    1.03x\nBenchmarkReadSlice1000Uint8s          317.36     31297.42   98.62x\nBenchmarkReadSlice1000Int16s          105.24       189.94    1.80x\nBenchmarkReadSlice1000Uint16s         148.69       171.38    1.15x\nBenchmarkReadSlice1000Int32s          204.41       296.06    1.45x\nBenchmarkReadSlice1000Uint32s         146.06       300.22    2.06x\nBenchmarkReadSlice1000Int64s          187.20       599.52    3.20x\nBenchmarkReadSlice1000Uint64s         252.05       577.58    2.29x\nBenchmarkWriteSlice1000Int8s          310.16       440.25    1.42x\nBenchmarkWriteSlice1000Uint8s        9232.01     33846.01    3.67x\nBenchmarkWriteSlice1000Int16s         155.44       251.55    1.62x\nBenchmarkWriteSlice1000Uint16s        113.43       206.91    1.82x\nBenchmarkWriteSlice1000Int32s         184.81       383.27    2.07x\nBenchmarkWriteSlice1000Uint32s        173.38       380.65    2.20x\nBenchmarkWriteSlice1000Int64s         244.65       760.44    3.11x\nBenchmarkWriteSlice1000Uint64s        252.28       757.62    3.00x\nBenchmarkReadStruct                    19.61        34.39    1.75x\nBenchmarkWriteStruct                   18.89        39.48    2.09x\nBenchmarkReadStructCustom              79.64        91.66    1.15x\nBenchmarkWriteStructCustom            242.41       322.56    1.33x\nBenchmarkReadSliceStruct                0.00       295.43     infx\nBenchmarkReadSliceStructCustom        143.93       178.60    1.24x\nBenchmarkWriteSliceStructCustom       402.88       867.11    2.15x\nBenchmarkReadArrayStruct              130.43       296.48    2.27x\nBenchmarkWriteArrayStruct             123.24       573.66    4.65x\nBenchmarkReadInts                      22.94        25.04    1.09x\nBenchmarkWriteInts                     23.75        21.91    0.92x\n\nbenchmark                          old bytes    new bytes    delta\nBenchmarkReadSlice1000Int8s             1032            8  -99.22%\nBenchmarkReadSlice1000Uint8s            1032            0  -100.00%\nBenchmarkReadSlice1000Int16s            2056            8  -99.61%\nBenchmarkReadSlice1000Uint16s           2056            8  -99.61%\nBenchmarkReadSlice1000Int32s            4104            8  -99.81%\nBenchmarkReadSlice1000Uint32s           4104            8  -99.81%\nBenchmarkReadSlice1000Int64s            8200           16  -99.80%\nBenchmarkReadSlice1000Uint64s           8200           16  -99.80%\nBenchmarkWriteSlice1000Int8s            1032            8  -99.22%\nBenchmarkWriteSlice1000Uint8s              8            0  -100.00%\nBenchmarkWriteSlice1000Int16s           2056            8  -99.61%\nBenchmarkWriteSlice1000Uint16s          2056            8  -99.61%\nBenchmarkWriteSlice1000Int32s           4104            8  -99.81%\nBenchmarkWriteSlice1000Uint32s          4104            8  -99.81%\nBenchmarkWriteSlice1000Int64s           8200           16  -99.80%\nBenchmarkWriteSlice1000Uint64s          8200           16  -99.80%\nBenchmarkReadStruct                      247           99  -59.92%\nBenchmarkWriteStruct                     256           99  -61.33%\nBenchmarkReadStructCustom                  8            8    0.00%\nBenchmarkWriteStructCustom                 8            8    0.00%\nBenchmarkReadSliceStruct                 123          115   -6.50%\nBenchmarkReadSliceStructCustom             8            8    0.00%\nBenchmarkWriteSliceStructCustom            8            8    0.00%\nBenchmarkReadArrayStruct                8248          115  -98.61%\nBenchmarkWriteArrayStruct               8256          115  -98.61%\nBenchmarkReadInts                         64           64    0.00%\nBenchmarkWriteInts                        64           64    0.00%\n\n(old is hg tip, new is my fork. BenchmarkReadSliceStruct shows more time because it's not supported on current hg tip)\n\nAs you can see, there are very significant speed ups and the amount of bytes allocated has dramatically decreased. In fact, for structs with a low number of fields, my code beats custom written methods when it comes to reading (because I wrote the custom methods using io.ReadFull, as I would expect users to do, and my encoding/binary code uses an optimized version of io.ReadAtLeast). By rewriting ByteOrder methods in assembler, I was also able to provide optimal implementations for the byte swapping operations, which increase performance of custom encode/decode methods.\n\nI added around 1300 lines of code (~500 are new tests and benchmarks). I don't know if that's worth for you, but for me it certainly is. Now I'm able to use encoding/binary directly, since the overhead is now only ~2-3x rather than ~8-10x and its memory usage has gone down by ~99%. In fact, I could probably make that gap in performance smaller by using unsafe, but I'll leave that for another day.\n\nI just want to clarify that I have no intention to submit this code as CL. In total, it took me around 5 hours to perform these optimizations, way less time than I spent arguing about the code that I wrote. That's, unfortunately, a waste of your and (most importantly to me) my time. In the future I will only submit CLs which are a PITA to maintain in a fork (e.g. changes to the toolchain or the runtime).\n\nRegards,\nAlberto",
			"disapproval": false,
			"date": "2013-08-10 16:13:14.659310",
			"approval": false
		},
		{
			"sender": "r@golang.org",
			"recipients": [
				"reply@codereview-hr.appspotmail.com"
			],
			"text": "Those numbers are impressive.\r\n\r\nI am sorry you feel frustrated and especially sorry you feel you\r\nwasted time talking to us. But as contribute.html tries to make clear,\r\nit's better to discuss your design before coding, not once the code is\r\nalready written:\r\n\r\n\"Before undertaking to write something new for the Go project, send\r\nmail to the mailing list to discuss what you plan to do. This gives\r\neveryone a chance to validate the design, helps prevent duplication of\r\neffort, and ensures that the idea fits inside the goals for the\r\nlanguage and tools. It also guarantees that the design is sound before\r\ncode is written; the code review tool is not the place for high-level\r\ndiscussions.\"\r\n\r\nWe spent a lot of time at cross-purposes within the code review\r\nprocess, even as the code was being actively developed. With some\r\nconversation up front, much of that could have been avoided. The\r\nprocess did let you down, but I feel it could have gone better. The\r\nend result might have been the same, but it would at least have been\r\nmore efficient for all concerned.\r\n\r\nI say this in the manner of a post-mortem, a lesson for us and for\r\nfuture contributors.\r\n\r\n-rob\r\n",
			"disapproval": false,
			"date": "2013-08-10 23:31:56.360940",
			"approval": false
		}
	],
	"owner_email": "alberto.garcia.hierro@gmail.com",
	"private": false,
	"base_url": "",
	"owner": "Hierro",
	"subject": "code review 12521043: encoding/binary: Reduce memory allocations and add fast...",
	"created": "2013-08-06 10:24:14.847990",
	"patchsets": [
		1,
		3001,
		6001,
		18001,
		20003
	],
	"modified": "2013-08-10 23:31:56.646090",
	"closed": true,
	"issue": 12521043
}